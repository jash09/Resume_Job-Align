{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am well, thank you for asking. How are you?\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ['API_KEY'])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content('Hi How are you doing')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pdf_setup(file_path):\n",
    "    text = ''\n",
    "    if file_path is not None:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extractText()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jash Shah      Baltimore, MD|+1 (667) 600-9472 |jshah48@jh.edu|github.com/jash09|www.linkedin.com/in/jashshah09  EDUCATION Johns Hopkins University, USA                                                                                                      August 2023 – May 2025(Expected) Master of Science in Engineering in Computer Science Relevant Courses – Algorithms, Machine Translation, Natural Language Processing                                                                                                                          University of Mumbai, India                                                                                                                           August 2018 – July 2022 Bachelor of Technology in Information Technology           (CGPA 8.81/10) Relevant Courses - Data Structures and Algorithms, Operating Systems, Cloud Computing, Object Oriented Programming, DevOps.  TECHNICAL SKILLS AND CERTIFICATIONS• Programming languages - Python, C++, C, Java, JavaScript, HTML, CSS, PHP, Shell. • Tools, technologies, and frameworks - Scikit-learn, Pandas, NumPy, Matplotlib, NLTK, TensorFlow, Django, PyTorch, Keras, OpenCV, Bootstrap, Android Studio, Flutter, ReactJS, Docker, Flask, AWS(EC2), Git, JIRA, Agile, Scrum, Linux, MS Excel. • Databases and BI tools – MySQL, PostgreSQL, MongoDB, AWS(S3), GCP, Tableau, PowerBI, Grafana. • Certifications - Data Science and Deep Learning Specialization offered on Coursera, and Android App Development offered on Udemy.  PROFESSIONAL EXPERIENCE The Johns Hopkins University | Lead Course Assistant – Gateway Computing: Python                                                   August 2023 – Present • Mentoring, supporting, and helping students with coursework in the classroom and conducting quizzes and project discussion classes. • Reviewing, editing, and grading student assessments and modules and helping with teaching logistics.  Forcepoint, Mumbai, India | Software Engineer - I                                                                           July 2022 – June 2023 • Implemented a robust Certificate Revocation Check feature in Python, bolstering Cloud Web Proxy security. Effectively detected and blocked expired or invalid certificates for accessed websites, substantially enhancing system integrity. • Improved the code coverage by 12% by developing test cases for various components of the Cloud Web Proxy. • Built a Quick Escalation Issue (EI) Analyzer that uses a Slack bot as a utility to provide most similar EI JIRA tickets to the EI ticket that is created. Utilized a combination of BERT and cosine similarity for the same which improved the EI ticket closing time by about 20%.  • Was awarded with the Best Business and Engineering Value award at the Forcepoint Global Hackathon 2022 for the same.   Tech Mahindra, Mumbai, India | Data Analyst intern                                                                  June 2021 – October 2021 • Built data warehouses using Snowflake and ETL pipelines that ingested customer data from a web application with 10,000 daily users. • Increased the sales demand forecasting accuracy by 7% by creating dashboards of sales data using PowerBI and Tableau.  InVideo, Mumbai, India | Data Science intern                                                               November 2020 – April 2021 • Optimized the SQL query performance and reduced the processing time by about 70% and devised a User Trend Analytics and Prediction System to analyze and predict the usage of about 150 users. • Introduced a custom module to detect nouns from sentences with the help of Named Entity Recognition for retrieving APIs and increased the efficiency by 35%.  PROJECTS  Crypto-currency Investment Scorer | Python, ML, NLP, Yahoo Finance API, Tweepy API, Flask | [GitHub] • Designed and created a model that combined LSTM for predicting the price of Bitcoin and TextBlob for predicting the sentiment of the market at that point based on the most recent tweets. • Applied quantitative strategies that gave an investment score with an accuracy of 78% and deployed the model using Flask.  A music web-app | ReactJS, Material-UI, Indexed DB, and YouTube API | [GitHub] • Developed a web-application using ReactJS that allows users to listen and download latest trending music using YouTube API. • Enabled distinct features such as media player with all controls, recommended songs, shuffle playlist, repeat playlist, sleep timer, trending songs of the country, offline download using Indexed DB and Google-o-auth login.  Covid-19 Outbreak Prediction| Python, Scikit-learn, WebPlotDigitizer | [paper link] • Implemented and fine-tuned time-series forecasting models like ARIMA, ARMA and Holt Winters Exponential Smoothing for predicting the number of Covid-19 cases for next 5 days for 10 most densely populated countries. • Obtained highest accuracy of 99.93% for one country and average accuracy of 87%.  A Method for Waste Segregation using Convolutional Neural Networks | Python, TensorFlow and FastAI | [paper link] • Engineered a custom CNN model that gave an accuracy of 94.9% and performed a comparative study of models such as VGG16 and ResNet-34 for classification of waste as organic and recyclable.  PUBLICATIONS• \"Bitcoin Investment Classifier using Machine Learning and Sentimental Analysis,\" ICICICT 2022. Published in IEEE Xplore [paper link]. • \"A Method for Waste Segregation using Convolutional Neural Networks,\" ICAECT 2022. Published in IEEE Xplore [paper link]. • \"Outbreak Prediction of COVID-19 for Dense and Populated Countries Using ML\". Ann Data. Sci. 8, 1–19 (2021) [paper link]. • \"Distributed Deep Learning and its Application in Geospatial Analytics\". CRC Press, Taylor & Francis Group [book chapter]. \n"
     ]
    }
   ],
   "source": [
    "print(input_pdf_setup('Resume_Jash_09_SDE.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_response(input,pdf_content,prompt):\n",
    "    model=genai.GenerativeModel('gemini-pro')\n",
    "    response=model.generate_content([input,pdf_content,prompt])\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt1 = \"\"\"\n",
    " You are an experienced Technical Human Resource Manager,your task is to review the provided resume against the job description. \n",
    "  Please share your professional evaluation on whether the candidate's profile aligns with the role. \n",
    " Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "About the job\n",
    "Data Analyst Internship -Data Science & Gen-AI Team\n",
    "\n",
    "At Peerlogic, we're pioneering the integration of artificial intelligence to revolutionize communication within the healthcare community. Guided by our core values of Curiosity, Accountability, Grit, and Enthusiasm (CAGE), we strive for excellence in innovating AI-driven solutions that improve, streamline, and transform professional interactions in medical settings. We're on the hunt for a Data Analysis II Intern to join our Data Science and Gen-AI team, a role critical to our mission of shaping the future of healthcare communication.\n",
    "\n",
    "Location: Scottsdale, Arizona\n",
    "\n",
    "Duration: 6 months, with potential for extension or transition to a full-time position\n",
    "\n",
    "Role Overview\n",
    "\n",
    "As a Data Analysis Intern, you will dive deep into the intricacies of AI and data science, contributing directly to projects at the cutting edge of technology and healthcare. Reporting to Dr. Amir Yazdavar, PhD, Principal Data Scientist, your work will support the development of AI solutions that embody our CAGE values, pushing the boundaries of what's possible in healthcare communication.\n",
    "\n",
    "Key Responsibilities\n",
    "\n",
    "    Conduct advanced data analysis, utilizing statistical software and machine learning algorithms to extract insights from complex healthcare datasets.\n",
    "    Collaborate with our AI development team to refine and enhance AI models, ensuring they meet the high standards required for healthcare applications.\n",
    "    Engage in the entire lifecycle of AI solution development, from initial data collection and analysis to implementation and feedback collection.\n",
    "    Develop and present clear, actionable insights to team members and stakeholders, aiding in decision-making and strategy development.\n",
    "    Contribute to the creation of a robust knowledge base by documenting findings, methodologies, and best practices.\n",
    "\n",
    "Qualifications\n",
    "\n",
    "    Pursuing or recently completed a degree in Data Science, Computer Science, Statistics, Mathematics, or related field, with a strong academic record.\n",
    "    Demonstrated experience with data analysis and statistical tools (e.g., Python, R), and a keen interest in AI and machine learning.\n",
    "    A proactive learner with a knack for problem-solving and a passion for healthcare innovation.\n",
    "    Exceptional communication skills, with the ability to convey complex ideas effectively to a diverse audience.\n",
    "    Alignment with Peerlogic’s core values of Curiosity, Accountability, Grit, and Enthusiasm, and a strong desire to make a positive impact in healthcare.\n",
    "\n",
    "What You'll Gain:\n",
    "\n",
    "    A competitive stipend and flexible scheduling to accommodate your educational commitments.\n",
    "    Hands-on experience in a fast-paced startup environment, working on groundbreaking projects at the intersection of AI and healthcare.\n",
    "    Direct mentorship from Dr. Amir Yazdavar and the opportunity to learn from a team of experienced professionals.\n",
    "    Access to a network of industry professionals and potential for future employment opportunities within Peerlogic.\n",
    "\n",
    "Interested candidates should apply with a resume, cover letter detailing their interest and fit for the role, and any relevant project samples or portfolios.\n",
    "\n",
    "Peerlogic is an equal-opportunity employer dedicated to building a diverse and inclusive team. We encourage applications from all qualified individuals who share our vision for improving healthcare communication. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the resume and trying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_structure = \"\"\"Objective:  Parse a text-formatted resume efficiently and extract diverse applicant's data into a structured JSON format that adheres to the provided TypeScript interface schema.\n",
    "\n",
    "Input: Text-formatted applicant's resume.\n",
    "\n",
    "Steps:\n",
    "1. Analyze Structure: Examine the text-formatted resume to understand its organization and identify key sections (e.g., education, experience, skills).\n",
    "2. Convert to JSON: Map the extracted information to the corresponding fields in the schema, creating a structured JSON representation.\n",
    "3. Optimize Output: Ensure the JSON is well-formatted, error-free, and handles missing values appropriately.\n",
    "4. Handle Variations: Account for different resume styles and formatting to accurately extract relevant data.\n",
    "\n",
    "Consider following TypeScript Interface for JSON schema:\n",
    "```\n",
    "interface Media {\n",
    "  linkedin: string;\n",
    "  github: string;\n",
    "  devpost: string;\n",
    "  medium: string;\n",
    "  leetcode: string;\n",
    "  dagshub: string;\n",
    "  kaggle: string;\n",
    "  instagram: string;\n",
    "}\n",
    "\n",
    "interface Education {\n",
    "  degree: string;\n",
    "  university: string;\n",
    "  from: string;\n",
    "  to: string;\n",
    "  grade?: string;\n",
    "  coursework?: string[];\n",
    "}\n",
    "\n",
    "interface SkillSection {\n",
    "  name: string;\n",
    "  skills: string[];\n",
    "}\n",
    "\n",
    "interface Work {\n",
    "  role: string;\n",
    "  company: string;\n",
    "  from: string;\n",
    "  to: string;\n",
    "  description: string[];\n",
    "}\n",
    "\n",
    "interface Project {\n",
    "  name: string;\n",
    "  type: string;\n",
    "  link?: string;\n",
    "  from: string;\n",
    "  to: string;\n",
    "  description: string[];\n",
    "}\n",
    "\n",
    "interface Certification {\n",
    "  name: string;\n",
    "  by: string;\n",
    "  link: string;\n",
    "}\n",
    "\n",
    "interface Achievements {\n",
    "  [index: number]: string;\n",
    "}\n",
    "\n",
    "interface RESUME_DATA_SCHEMA {\n",
    "  name: string;\n",
    "  summary: string;\n",
    "  phone: string;\n",
    "  email: string;\n",
    "  media: Media;\n",
    "  education: Education[];\n",
    "  skill_section: SkillSection[];\n",
    "  work_experience: Work[];\n",
    "  projects: Project[];\n",
    "  certifications: Certification[];\n",
    "  achievements: Achievements;\n",
    "}\n",
    "```\n",
    "\n",
    "Desired Output: Write the Well-formatted JSON adhering to the RESUME_DATA_SCHEMA schema, handling missing values with empty strings or \"None\".\n",
    "<JSON_OUTPUT_ACCORDING_TO_RESUME_DATA_SCHEMA>\n",
    "\n",
    "The results should contain valid JSON only, without any delimiter or characters making invalid JSON format.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel('gemini-pro')\n",
    "response=model.generate_content([prompt_structure,input_pdf_setup('Resume_Jash_09_SDE.pdf')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"Jash Shah\",\n",
      "  \"summary\": null,\n",
      "  \"phone\": \"+1 (667) 600-9472\",\n",
      "  \"email\": \"jshah48@jh.edu\",\n",
      "  \"media\": {\n",
      "    \"linkedin\": \"www.linkedin.com/in/jashshah09\",\n",
      "    \"github\": \"github.com/jash09\",\n",
      "    \"devpost\": null,\n",
      "    \"medium\": null,\n",
      "    \"leetcode\": null,\n",
      "    \"dagshub\": null,\n",
      "    \"kaggle\": null,\n",
      "    \"instagram\": null\n",
      "  },\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"Master of Science in Engineering in Computer Science\",\n",
      "      \"university\": \"Johns Hopkins University, USA\",\n",
      "      \"from\": \"August 2023\",\n",
      "      \"to\": \"May 2025(Expected)\",\n",
      "      \"grade\": null,\n",
      "      \"coursework\": [\n",
      "        \"Algorithms\",\n",
      "        \"Machine Translation\",\n",
      "        \"Natural Language Processing\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Bachelor of Technology in Information Technology\",\n",
      "      \"university\": \"University of Mumbai, India\",\n",
      "      \"from\": \"August 2018\",\n",
      "      \"to\": \"July 2022\",\n",
      "      \"grade\": \"8.81/10\",\n",
      "      \"coursework\": [\n",
      "        \"Data Structures and Algorithms\",\n",
      "        \"Operating Systems\",\n",
      "        \"Cloud Computing\",\n",
      "        \"Object Oriented Programming\",\n",
      "        \"DevOps\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"skill_section\": [\n",
      "    {\n",
      "      \"name\": \"Programming languages\",\n",
      "      \"skills\": [\n",
      "        \"Python\",\n",
      "        \"C++\",\n",
      "        \"C\",\n",
      "        \"Java\",\n",
      "        \"JavaScript\",\n",
      "        \"HTML\",\n",
      "        \"CSS\",\n",
      "        \"PHP\",\n",
      "        \"Shell\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tools, technologies, and frameworks\",\n",
      "      \"skills\": [\n",
      "        \"Scikit-learn\",\n",
      "        \"Pandas\",\n",
      "        \"NumPy\",\n",
      "        \"Matplotlib\",\n",
      "        \"NLTK\",\n",
      "        \"TensorFlow\",\n",
      "        \"Django\",\n",
      "        \"PyTorch\",\n",
      "        \"Keras\",\n",
      "        \"OpenCV\",\n",
      "        \"Bootstrap\",\n",
      "        \"Android Studio\",\n",
      "        \"Flutter\",\n",
      "        \"ReactJS\",\n",
      "        \"Docker\",\n",
      "        \"Flask\",\n",
      "        \"AWS(EC2)\",\n",
      "        \"Git\",\n",
      "        \"JIRA\",\n",
      "        \"Agile\",\n",
      "        \"Scrum\",\n",
      "        \"Linux\",\n",
      "        \"MS Excel\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Databases and BI tools\",\n",
      "      \"skills\": [\n",
      "        \"MySQL\",\n",
      "        \"PostgreSQL\",\n",
      "        \"MongoDB\",\n",
      "        \"AWS(S3)\",\n",
      "        \"GCP\",\n",
      "        \"Tableau\",\n",
      "        \"PowerBI\",\n",
      "        \"Grafana\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"role\": \"Lead Course Assistant – Gateway Computing: Python\",\n",
      "      \"company\": \"The Johns Hopkins University\",\n",
      "      \"from\": \"August 2023\",\n",
      "      \"to\": \"Present\",\n",
      "      \"description\": [\n",
      "        \"Mentoring, supporting, and helping students with coursework in the classroom and conducting quizzes and project discussion classes.\",\n",
      "        \"Reviewing, editing, and grading student assessments and modules and helping with teaching logistics.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Software Engineer - I\",\n",
      "      \"company\": \"Forcepoint, Mumbai, India\",\n",
      "      \"from\": \"July 2022\",\n",
      "      \"to\": \"June 2023\",\n",
      "      \"description\": [\n",
      "        \"Implemented a robust Certificate Revocation Check feature in Python, bolstering Cloud Web Proxy security. Effectively detected and blocked expired or invalid certificates for accessed websites, substantially enhancing system integrity.\",\n",
      "        \"Improved the code coverage by 12% by developing test cases for various components of the Cloud Web Proxy.\",\n",
      "        \"Built a Quick Escalation Issue (EI) Analyzer that uses a Slack bot as a utility to provide most similar EI JIRA tickets to the EI ticket that is created. Utilized a combination of BERT and cosine similarity for the same which improved the EI ticket closing time by about 20%.\",\n",
      "        \"Was awarded with the Best Business and Engineering Value award at the Forcepoint Global Hackathon 2022 for the same.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Data Analyst intern\",\n",
      "      \"company\": \"Tech Mahindra, Mumbai, India\",\n",
      "      \"from\": \"June 2021\",\n",
      "      \"to\": \"October 2021\",\n",
      "      \"description\": [\n",
      "        \"Built data warehouses using Snowflake and ETL pipelines that ingested customer data from a web application with 10,000 daily users.\",\n",
      "        \"Increased the sales demand forecasting accuracy by 7% by creating dashboards of sales data using PowerBI and Tableau.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Data Science intern\",\n",
      "      \"company\": \"InVideo, Mumbai, India\",\n",
      "      \"from\": \"November 2020\",\n",
      "      \"to\": \"April 2021\",\n",
      "      \"description\": [\n",
      "        \"Optimized the SQL query performance and reduced the processing time by about 70% and devised a User Trend Analytics and Prediction System to analyze and predict the usage of about 150 users.\",\n",
      "        \"Introduced a custom module to detect nouns from sentences with the help of Named Entity Recognition for retrieving APIs and increased the efficiency by 35%.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"name\": \"Crypto-currency Investment Scorer\",\n",
      "      \"type\": \"Python, ML, NLP, Yahoo Finance API, Tweepy API, Flask\",\n",
      "      \"link\": \"GitHub\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Designed and created a model that combined LSTM for predicting the price of Bitcoin and TextBlob for predicting the sentiment of the market at that point based on the most recent tweets.\",\n",
      "        \"Applied quantitative strategies that gave an investment score with an accuracy of 78% and deployed the model using Flask.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"A music web-app\",\n",
      "      \"type\": \"ReactJS, Material-UI, Indexed DB, and YouTube API\",\n",
      "      \"link\": \"GitHub\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Developed a web-application using ReactJS that allows users to listen and download latest trending music using YouTube API.\",\n",
      "        \"Enabled distinct features such as media player with all controls, recommended songs, shuffle playlist, repeat playlist, sleep timer, trending songs of the country, offline download using Indexed DB and Google-o-auth login.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Covid-19 Outbreak Prediction\",\n",
      "      \"type\": \"Python, Scikit-learn, WebPlotDigitizer\",\n",
      "      \"link\": \"paper link\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Implemented and fine-tuned time-series forecasting models like ARIMA, ARMA and Holt Winters Exponential Smoothing for predicting the number of Covid-19 cases for next 5 days for 10 most densely populated countries.\",\n",
      "        \"Obtained highest accuracy of 99.93% for one country and average accuracy of 87%.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"A Method for Waste Segregation using Convolutional Neural Networks\",\n",
      "      \"type\": \"Python, TensorFlow and FastAI\",\n",
      "      \"link\": \"paper link\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Engineered a custom CNN model that gave an accuracy of 94.9% and performed a comparative study of models such as VGG16 and ResNet-34 for classification of waste as organic and recyclable.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Data Science and Deep Learning Specialization\",\n",
      "      \"by\": \"Coursera\",\n",
      "      \"link\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Android App Development\",\n",
      "      \"by\": \"Udemy\",\n",
      "      \"link\": null\n",
      "    }\n",
      "  ],\n",
      "  \"achievements\": [\n",
      "    \"“Bitcoin Investment Classifier using Machine Learning and Sentimental Analysis,” ICICICT 2022. Published in IEEE Xplore\",\n",
      "    \"“A Method for Waste Segregation using Convolutional Neural Networks,” ICAECT 2022. Published in IEEE Xplore\",\n",
      "    \"“Outbreak Prediction of COVID-19 for Dense and Populated Countries Using ML”. Ann Data. Sci. 8, 1–19 (2021)\",\n",
      "    \"“Distributed Deep Learning and its Application in Geospatial Analytics”. CRC Press, Taylor & Francis Group [book chapter]\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_job_description = \"\"\"I will provide a job and company description for your analysis. In your analysis you have to identify the key words, expertise & requirements the job demands, but also from the company description.\n",
    "\n",
    "Your task is consider it to meticulously assess the information and furnish the details like Job title, Keywords(key words are expertise & requirements the job demands), Job purpose, Job duties and responsibilities, Required qualifications, Preferred qualifications, Company name and Company details(points which include overview, mission, values or way of working) in bulleted points. \n",
    "\n",
    "Provide them only in JSON format - which is easily parse by any json parser - with following keys:\n",
    "title, keywords, purpose, duties_responsibilities, required_qualifications, preferred_qualifications, company_name, company_info.\n",
    "\n",
    "Output:\n",
    "{\n",
    "    \"title\": \"RESULT\", \n",
    "    \"keywords\": \"RESULT\", \n",
    "    purpose: \"RESULT\", \n",
    "    duties_responsibilities: \"RESULT\", \n",
    "    required_qualifications: \"RESULT\", \n",
    "    preferred_qualifications: \"RESULT\", \n",
    "    company_name: \"RESULT\",\n",
    "    company_info: \"RESULT\",\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_jd = model.generate_content([prompt_job_description, input_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Data Analysis II Intern\",\n",
      "    \"keywords\": \"Data Science, Gen-AI, AI, Healthcare Communication, Statistical Analysis, Machine Learning, Data Analysis, AI Solution Development, Healthcare Innovation\",\n",
      "    \"purpose\": \"Drive data-driven decision-making and contribute to the development of AI solutions in the healthcare industry.\",\n",
      "    \"duties_responsibilities\": \"- Conduct advanced data analysis using statistical software and machine learning algorithms.\\n- Collaborate with the AI development team to refine and enhance AI models.\\n- Participate in the entire lifecycle of AI solution development.\\n- Develop and present clear and actionable insights to team members and stakeholders.\\n- Contribute to the creation of a robust knowledge base.\",\n",
      "    \"required_qualifications\": \"- Pursuing or recently completed a degree in Data Science, Computer Science, Statistics, or related field.\\n- Demonstrated experience with data analysis and statistical tools like Python or R.\\n- Keen interest in AI and machine learning.\\n- Proactive learner with strong problem-solving and healthcare innovation passion.\\n- Exceptional communication skills.\",\n",
      "    \"preferred_qualifications\": \"- Alignment with Peerlogic’s core values: Curiosity, Accountability, Grit, and Enthusiasm.\\n- Strong desire to make a positive impact in healthcare.\",\n",
      "    \"company_name\": \"Peerlogic\",\n",
      "    \"company_info\": \"- Overview: Pioneer in integrating AI to revolutionize communication in the healthcare industry.\\n- Mission: To improve, streamline, and transform professional interactions in medical settings.\\n- Values: Curiosity, Accountability, Grit, and Enthusiasm (CAGE).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response_jd.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_resume_align_prompt = \"\"\"I will provide a job description and my details in JSON format.\n",
    "Your task is to analyze and match my details with the job's requirements.\n",
    "Then, you need to create the best possible resume in JSON format to align my details with the job description.\n",
    "\n",
    "Instructions:\n",
    "- Include only 3 work and project experiences. each experience with 3 bulleted points closely aligned with job details. It very important you follow this.\n",
    "- Use quantifiable impacts for each bullet point.\n",
    "- Rewrite job highlights using the STAR methodology without explicitly mentioning STAR.\n",
    "- Employ STRONG action verbs showcasing soft skills.\n",
    "- Maintain truthfulness and objectivity in listing experience.\n",
    "- Format experience points as 'Did X by doing Y accomplish Z'.\n",
    "- Prioritize specificity - with respect to job - over generality.\n",
    "- Proofread and Correct spelling and grammar errors.\n",
    "- Aim for clear expression over impressiveness.\n",
    "- Prefer active voice over passive voice.\n",
    "- Omit a summary about the candidate.\n",
    "\n",
    "Output the response in JSON format only - below I have given example delimited by ```. Output must be easily parse by python's json parser. Extract all mentioned properties in given example without changing their names.\n",
    "```\n",
    "{\n",
    "  \"personal\": { \"name\": \"\", \"phone\": \"+1 111-222-3333\", \"email\": \"\", \"github\": \"\", \"linkedin\": \"\"\n",
    "  },\n",
    "  \"education\": [\n",
    "    { \"degree\": \"\", \"university\": \"\", \"from\": \"\", \"to\": \"\", \"grade\": \"\", \"coursework\": [] },\n",
    "    ...\n",
    "  ],\n",
    "  \"skill_section\": [\n",
    "    { \"name\": \"\", \"skills\": [] }\n",
    "    ...\n",
    "  ],\n",
    "  \"work\": [\n",
    "    { \"role\": \"\", \"company\": \"<only company name, no location>\", \"location\": \"\", \"from\": \"\", \"to\": \"\", \"description\": [] },\n",
    "    ...\n",
    "  ],\n",
    "  \"projects\": [\n",
    "    { \"name\": \"\", \"link\": \"\", \"from\": \"\", \"to\": \"\", \"description\": [] },\n",
    "    ...\n",
    "  ],\n",
    "  \"certifications\": [\n",
    "    {\"name\": \"\", \"issuer\": \"\", \"link\": \"\"},\n",
    "    ...\n",
    "  ],\n",
    "  \"achievements\": [\n",
    "    \"achievements_1\",\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_final = model.generate_content([job_resume_align_prompt, response_jd.text, response.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"personal\": {\n",
      "    \"name\": \"Jash Shah\",\n",
      "    \"phone\": \"+1 (667) 600-9472\",\n",
      "    \"email\": \"jshah48@jh.edu\",\n",
      "    \"github\": \"github.com/jash09\",\n",
      "    \"linkedin\": \"www.linkedin.com/in/jashshah09\"\n",
      "  },\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"Master of Science in Engineering in Computer Science\",\n",
      "      \"university\": \"Johns Hopkins University, USA\",\n",
      "      \"from\": \"August 2023\",\n",
      "      \"to\": \"May 2025(Expected)\",\n",
      "      \"grade\": null,\n",
      "      \"coursework\": [\n",
      "        \"Algorithms\",\n",
      "        \"Machine Translation\",\n",
      "        \"Natural Language Processing\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Bachelor of Technology in Information Technology\",\n",
      "      \"university\": \"University of Mumbai, India\",\n",
      "      \"from\": \"August 2018\",\n",
      "      \"to\": \"July 2022\",\n",
      "      \"grade\": \"8.81/10\",\n",
      "      \"coursework\": [\n",
      "        \"Data Structures and Algorithms\",\n",
      "        \"Operating Systems\",\n",
      "        \"Cloud Computing\",\n",
      "        \"Object Oriented Programming\",\n",
      "        \"DevOps\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"skill_section\": [\n",
      "    {\n",
      "      \"name\": \"Programming languages\",\n",
      "      \"skills\": [\n",
      "        \"Python\",\n",
      "        \"C++\",\n",
      "        \"C\",\n",
      "        \"Java\",\n",
      "        \"JavaScript\",\n",
      "        \"HTML\",\n",
      "        \"CSS\",\n",
      "        \"PHP\",\n",
      "        \"Shell\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tools, technologies, and frameworks\",\n",
      "      \"skills\": [\n",
      "        \"Scikit-learn\",\n",
      "        \"Pandas\",\n",
      "        \"NumPy\",\n",
      "        \"Matplotlib\",\n",
      "        \"NLTK\",\n",
      "        \"TensorFlow\",\n",
      "        \"Django\",\n",
      "        \"PyTorch\",\n",
      "        \"Keras\",\n",
      "        \"OpenCV\",\n",
      "        \"Bootstrap\",\n",
      "        \"Android Studio\",\n",
      "        \"Flutter\",\n",
      "        \"ReactJS\",\n",
      "        \"Docker\",\n",
      "        \"Flask\",\n",
      "        \"AWS(EC2)\",\n",
      "        \"Git\",\n",
      "        \"JIRA\",\n",
      "        \"Agile\",\n",
      "        \"Scrum\",\n",
      "        \"Linux\",\n",
      "        \"MS Excel\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Databases and BI tools\",\n",
      "      \"skills\": [\n",
      "        \"MySQL\",\n",
      "        \"PostgreSQL\",\n",
      "        \"MongoDB\",\n",
      "        \"AWS(S3)\",\n",
      "        \"GCP\",\n",
      "        \"Tableau\",\n",
      "        \"PowerBI\",\n",
      "        \"Grafana\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"work\": [\n",
      "    {\n",
      "      \"role\": \"Software Engineer - I\",\n",
      "      \"company\": \"Forcepoint, Mumbai, India\",\n",
      "      \"from\": \"July 2022\",\n",
      "      \"to\": \"June 2023\",\n",
      "      \"description\": [\n",
      "        \"Improved the code coverage by 12% by developing test cases for various components of the Cloud Web Proxy.\",\n",
      "        \"Built a Quick Escalation Issue (EI) Analyzer that uses a Slack bot and BERT algorithms to close EI tickets about 20% faster.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Data Analyst intern\",\n",
      "      \"company\": \"Tech Mahindra, Mumbai, India\",\n",
      "      \"from\": \"June 2021\",\n",
      "      \"to\": \"October 2021\",\n",
      "      \"description\": [\n",
      "        \"Increased the sales demand forecasting accuracy by 7% by creating dashboards of sales data using PowerBI and Tableau.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Data Science intern\",\n",
      "      \"company\": \"InVideo, Mumbai, India\",\n",
      "      \"from\": \"November 2020\",\n",
      "      \"to\": \"April 2021\",\n",
      "      \"description\": [\n",
      "        \"Optimized the SQL query performance and reduced the processing time by about 70%\",\n",
      "        \"Introduced a custom Named Entity Recognition module to increase efficiency by 35%.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"name\": \"Crypto-currency Investment Scorer\",\n",
      "      \"link\": \"github\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Designed and created a LSTM model for predicting the price of Bitcoin and TextBlob for predicting the market sentiment based on tweets.\",\n",
      "        \"Applied quantitative strategies that gave an investment score with an accuracy of 78%.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Covid-19 Outbreak Prediction\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Obtained highest accuracy of 99.93% for one country and average accuracy of 87% using time-series forecasting models.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"A Method for Waste Segregation using Convolutional Neural Networks\",\n",
      "      \"from\": null,\n",
      "      \"to\": null,\n",
      "      \"description\": [\n",
      "        \"Engineered a CNN model that gave an accuracy of 94.9% for classification of waste.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Data Science and Deep Learning Specialization\",\n",
      "      \"by\": \"Coursera\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Android App Development\",\n",
      "      \"by\": \"Udemy\"\n",
      "    }\n",
      "  ],\n",
      "  \"achievements\": [\n",
      "    \"“Bitcoin Investment Classifier using Machine Learning and Sentimental Analysis,” ICICICT 2022. Published in IEEE Xplore\",\n",
      "    \"“A Method for Waste Segregation using Convolutional Neural Networks,” ICAECT 2022. Published in IEEE Xplore\",\n",
      "    \"“Outbreak Prediction of COVID-19 for Dense and Populated Countries Using ML”. Ann Data. Sci. 8, 1–19 (2021)\",\n",
      "    \"“Distributed Deep Learning and its Application in Geospatial Analytics”. CRC Press, Taylor & Francis Group [book chapter]\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response_final.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jashshah/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/jashshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise\n",
    "# from zlm.utils.utils import key_value_chunking\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(list_of_strings):\n",
    "    \"\"\"Removes strings containing URLs from a list using regular expressions.\"\"\"\n",
    "    filtered_list = [string for string in list_of_strings if not re.search(r\"https?://\\S+\", string)]\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> list:\n",
    "    \"\"\"Normalize the input text.\n",
    "\n",
    "    This function tokenizes the text, removes stopwords and punctuations, \n",
    "    and applies stemming.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of normalized words.\n",
    "    \"\"\"    \n",
    "    # Step 1: Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Step 2: Data Cleaning - Remove Stopwords and Punctuations \n",
    "    words = [re.sub('[^a-zA-Z]', '', word).lower() for word in words]\n",
    "\n",
    "    # Step 3: Remove empty tokens\n",
    "    words = [word for word in words if len(word)] \n",
    "\n",
    "    # Step 4: Remove Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Step 5: Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    #STEP 3 : LEMMATIZATION\n",
    "    # lemmatizer=WordNetLemmatizer()\n",
    "    # words=[lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_coefficient(document1: str, document2: str) -> float:\n",
    "    \"\"\"Calculate the overlap coefficient between two documents.\n",
    "\n",
    "    The overlap coefficient is a measure of the overlap between two sets, \n",
    "    and is defined as the size of the intersection divided by the smaller \n",
    "    of the size of the two sets.\n",
    "\n",
    "    Args:\n",
    "        document1 (str): The first document.\n",
    "        document2 (str): The second document.\n",
    "\n",
    "    Returns:\n",
    "        float: The overlap coefficient between the two documents.\n",
    "    \"\"\"    \n",
    "    # List the unique words in a document\n",
    "    words_in_document1 = set(normalize_text(document1))\n",
    "    words_in_document2 = set(normalize_text(document2))\n",
    "\n",
    "    # Find the intersection of words list of document1 & document2\n",
    "    intersection = words_in_document1.intersection(words_in_document2)\n",
    "\n",
    "    # Calculate overlap coefficient\n",
    "    try:\n",
    "        overlap_coefficient = float(len(intersection)) / min(len(words_in_document1), len(words_in_document2))\n",
    "    except ZeroDivisionError:\n",
    "        overlap_coefficient = 0.0\n",
    "\n",
    "    return overlap_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(document1: str, document2: str) -> float:\n",
    "    \"\"\"Calculate the Jaccard similarity between two documents.\n",
    "\n",
    "    The Jaccard similarity is a measure of the similarity between two sets, \n",
    "    and is defined as the size of the intersection divided by the size of \n",
    "    the union of the two sets.\n",
    "\n",
    "    Args:\n",
    "        document1 (str): The first document.\n",
    "        document2 (str): The second document.\n",
    "\n",
    "    Returns:\n",
    "        float: The Jaccard similarity between the two documents.\n",
    "    \"\"\"    \n",
    "    # List the unique words in a document\n",
    "    words_in_document1 = set(normalize_text(document1))\n",
    "    words_in_document2 = set(normalize_text(document2))\n",
    "\n",
    "    # Find the intersection of words list of document1 & document2\n",
    "    intersection = words_in_document1.intersection(words_in_document2)\n",
    "\n",
    "    # Find the union of words list of document1 & document2\n",
    "    union = words_in_document1.union(words_in_document2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    try:\n",
    "        jaccard_similarity = float(len(intersection)) / len(union)\n",
    "    except ZeroDivisionError:\n",
    "        jaccard_similarity = 0.0\n",
    "\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(document1: str, document2: str) -> float:\n",
    "    \"\"\"Calculate the cosine similarity between two documents.\n",
    "\n",
    "    Args:\n",
    "        document1 (str): The first document.\n",
    "        document2 (str): The second document.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two documents.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the documents into TF-IDF vectors\n",
    "    vectors = vectorizer.fit_transform([document1, document2])\n",
    "\n",
    "    cosine_similarity_score = pairwise.cosine_similarity(vectors[0], vectors[1])\n",
    "    # Calculate the cosine similarity between the two vectors\n",
    "    # cosine_similarity = np.dot(vectors[0], vectors[1].T) / (np.linalg.norm(vectors[0].toarray()) * np.linalg.norm(vectors[1].toarray()))\n",
    "\n",
    "    return cosine_similarity_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking overlap between job description and generated resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1568627450980392\n"
     ]
    }
   ],
   "source": [
    "print(overlap_coefficient(response_final.text, response_jd.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2028985507246377\n"
     ]
    }
   ],
   "source": [
    "print(overlap_coefficient(input_pdf_setup('Resume_Jash_09_SDE.pdf'), input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4322132233776859\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(response_final.text, input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.550808907843292\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(input_pdf_setup('Resume_Jash_09_SDE.pdf'), input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9864864864864865\n"
     ]
    }
   ],
   "source": [
    "print(overlap_coefficient(response.text, response_final.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8619690396704458\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(response.text, response_final.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vector_embedding_similarity(llm, document1: str, document2: str) -> float:\n",
    "#     document1 = key_value_chunking(json.loads(document1))\n",
    "#     document2 = key_value_chunking(json.loads(document2))\n",
    "    \n",
    "#     emb_1 = llm.get_embedding(document1, task_type=\"retrieval_query\")\n",
    "#     emb_2 = llm.get_embedding(document2, task_type=\"retrieval_query\")\n",
    "\n",
    "#     df1 = pd.DataFrame(emb_1.embedding.to_list())\n",
    "#     df2 = pd.DataFrame(emb_2.embedding.to_list())\n",
    "\n",
    "#     emb_sem = pairwise.cosine_similarity(df1, df2)\n",
    "\n",
    "#     return emb_sem.mean()\n",
    "\n",
    "\n",
    "#     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-ssd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
